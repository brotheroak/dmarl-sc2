{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 - Making RL PySC2 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Runnning 'Agent code' on jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
    "# This does not play well with jupyter notebook\n",
    "# So we will need to monkeypatch sys.argv\n",
    "\n",
    "\n",
    "import sys\n",
    "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import importlib\n",
    "import threading\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    flags.DEFINE_bool(\"render\", True, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", None, sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", True,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", False, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
    "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
    "\n",
    "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
    "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
    "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "    #                  \"Agent 1's race.\")\n",
    "    flags.DEFINE_string(\"agent\", \"TerranRLAgent\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"random\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"very_easy\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "\n",
    "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
    "    flags.mark_flag_as_required(\"map\")\n",
    "\n",
    "flags_defined = True\n",
    "\n",
    "def run_thread(agent_classes, players, map_name, visualize):\n",
    "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
    "  with sc2_env.SC2Env(\n",
    "      map_name=map_name,\n",
    "      players=players,\n",
    "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "          feature_screen=FLAGS.feature_screen_size,\n",
    "          feature_minimap=FLAGS.feature_minimap_size,\n",
    "          rgb_screen=FLAGS.rgb_screen_size,\n",
    "          rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "          action_space=FLAGS.action_space,\n",
    "          use_feature_units=FLAGS.use_feature_units),\n",
    "      step_mul=FLAGS.step_mul,\n",
    "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "      disable_fog=FLAGS.disable_fog,\n",
    "      visualize=visualize) as env:\n",
    "    env = available_actions_printer.AvailableActionsPrinter(env)\n",
    "    agents = [agent_cls() for agent_cls in agent_classes]\n",
    "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "    if FLAGS.save_replay:\n",
    "      env.save_replay(agent_classes[0].__name__)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \"\"\"Run an agent.\"\"\"\n",
    "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
    "  #stopwatch.sw.trace = FLAGS.trace\n",
    "\n",
    "  map_inst = maps.get(FLAGS.map)\n",
    "\n",
    "  agent_classes = []\n",
    "  players = []\n",
    "\n",
    "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
    "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "  #agent_classes.append(agent_cls)\n",
    "  agent_classes.append(TerranRLAgent)\n",
    "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
    "\n",
    "  if map_inst.players >= 2:\n",
    "    if FLAGS.agent2 == \"Bot\":\n",
    "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
    "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
    "    else:\n",
    "      agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
    "      agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "      agent_classes.append(agent_cls)\n",
    "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
    "\n",
    "  threads = []\n",
    "  for _ in range(FLAGS.parallel - 1):\n",
    "    t = threading.Thread(target=run_thread,\n",
    "                         args=(agent_classes, players, FLAGS.map, False))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
    "\n",
    "  for t in threads:\n",
    "    t.join()\n",
    "\n",
    "  if FLAGS.profile:\n",
    "    pass\n",
    "    #print(stopwatch.sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a RL PySC2 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        smart_action = smart_actions[random.randrange(0, len(smart_actions) - 1)]\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "   \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                if self.base_top_left:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [39, 45])\n",
    "                else:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [21, 24])\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                if self.base_top_left:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [39, 45])\n",
    "                else:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [21, 24])\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Defining Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        killed_unit_score = obs.observation.score_cumulative.killed_value_units\n",
    "        killed_building_score = obs.observation.score_cumulative.killed_value_structures\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if killed_unit_score > self.previous_killed_unit_score:\n",
    "            reward += KILL_UNIT_REWARD\n",
    "                \n",
    "        if killed_building_score > self.previous_killed_building_score:\n",
    "            reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                if self.base_top_left:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [39, 45])\n",
    "                else:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [21, 24])\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connecting All Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "    ACTION_ATTACK,\n",
    "]\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "\n",
    "    def transformLocation(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        killed_unit_score = obs.observation.score_cumulative.killed_value_units\n",
    "        killed_building_score = obs.observation.score_cumulative.killed_value_structures\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = rl_action\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformLocation(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                if self.base_top_left:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [39, 45])\n",
    "                else:\n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", [21, 24])\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Changing Attack Actions\n",
    "\n",
    "![Reduced attack points in Simple64 map](./images/reduced_attack_points_in_Simple64.png)\n",
    "image ref : Steven Brown's PySC2 blog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "]\n",
    "\n",
    "#for mm_x in range(0, 64):\n",
    "#    for mm_y in range(0, 64):\n",
    "#        smart_actions.append(ACTION_ATTACK + '_' + str(mm_x) + '_' + str(mm_y))\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 16 == 0 and (mm_y + 1) % 16 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 8) + '_' + str(mm_y - 8))\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "\n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        killed_unit_score = obs.observation.score_cumulative.killed_value_units\n",
    "        killed_building_score = obs.observation.score_cumulative.killed_value_structures\n",
    "        \n",
    "        current_state = [\n",
    "            supply_depot_count,\n",
    "            barracks_count,\n",
    "            supply_limit,\n",
    "            army_supply,\n",
    "        ]\n",
    "        \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = rl_action\n",
    "        \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformDistance(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformDistance(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x), int(y)))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adding Enemy Position to States\n",
    "\n",
    "![Reduced enemy position grid in Simple64 map](./images/reduced_enemy_position_grid_in_Simple64.png)\n",
    "image ref : Steven Brown's PySC2 blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_learning_data'\n",
    "SCORE_FILE = 'rlagent_learning_score'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_SELECT_SCV = 'selectscv'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_SELECT_BARRACKS = 'selectbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_SELECT_ARMY = 'selectarmy'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_SELECT_SCV,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_SELECT_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "    ACTION_SELECT_ARMY,\n",
    "]\n",
    "\n",
    "#for mm_x in range(0, 64):\n",
    "#    for mm_y in range(0, 64):\n",
    "#        smart_actions.append(ACTION_ATTACK + '_' + str(mm_x) + '_' + str(mm_y))\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 16 == 0 and (mm_y + 1) % 16 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 8) + '_' + str(mm_y - 8))\n",
    "\n",
    "KILL_UNIT_REWARD = 0.2\n",
    "KILL_BUILDING_REWARD = 0.5\n",
    "\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            # state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "\n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "\n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "\n",
    "        # q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        # q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "        q_target = r + self.gamma * self.q_table.loc[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        # self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgent, self).__init__()\n",
    "\n",
    "        self.base_top_left = None \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_killed_unit_score = 0\n",
    "        self.previous_killed_building_score = 0\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "\n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "    \n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.last():\n",
    "            self.qlearn.q_table.to_pickle(DATA_FILE + '.gz', 'gzip')\n",
    "            \n",
    "            scores_window.append(obs.reward)  # save most recent reward\n",
    "            win_rate = scores_window.count(1)/len(scores_window)*100\n",
    "            tie_rate = scores_window.count(0)/len(scores_window)*100\n",
    "            lost_rate = scores_window.count(-1)/len(scores_window)*100\n",
    "            \n",
    "            scores.append([win_rate, tie_rate, lost_rate])  # save most recent score(win_rate, tie_rate, lost_rate)\n",
    "            with open(SCORE_FILE + '.txt', \"wb\") as fp:\n",
    "                pickle.dump(scores, fp)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "\n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        killed_unit_score = obs.observation.score_cumulative.killed_value_units\n",
    "        killed_building_score = obs.observation.score_cumulative.killed_value_structures\n",
    "        \n",
    "#        current_state = np.zeros(5000)\n",
    "#        current_state[0] = supply_depot_count\n",
    "#        current_state[1] = barracks_count\n",
    "#        current_state[2] = supply_limit\n",
    "#        current_state[3] = army_supply\n",
    "#\n",
    "#        hot_squares = np.zeros(4096)        \n",
    "#        enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "#        for i in range(0, len(enemy_y)):\n",
    "#            y = int(enemy_y[i])\n",
    "#            x = int(enemy_x[i])\n",
    "#            \n",
    "#            hot_squares[((y - 1) * 64) + (x - 1)] = 1\n",
    "#        \n",
    "#        if not self.base_top_left:\n",
    "#            hot_squares = hot_squares[::-1]\n",
    "#        \n",
    "#        for i in range(0, 4096):\n",
    "#            current_state[i + 4] = hot_squares[i]\n",
    "\n",
    "        current_state = np.zeros(20)\n",
    "        current_state[0] = supply_depot_count\n",
    "        current_state[1] = barracks_count\n",
    "        current_state[2] = supply_limit\n",
    "        current_state[3] = army_supply\n",
    "\n",
    "        hot_squares = np.zeros(16)        \n",
    "        enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "        for i in range(0, len(enemy_y)):\n",
    "            y = int(math.ceil((enemy_y[i] + 1) / 16))\n",
    "            x = int(math.ceil((enemy_x[i] + 1) / 16))\n",
    "            \n",
    "            hot_squares[((y - 1) * 4) + (x - 1)] = 1\n",
    "        \n",
    "        if not self.base_top_left:\n",
    "            hot_squares = hot_squares[::-1]\n",
    "        \n",
    "        for i in range(0, 16):\n",
    "            current_state[i + 4] = hot_squares[i] \n",
    "            \n",
    "        if self.previous_action is not None:\n",
    "            reward = 0\n",
    "                \n",
    "            if killed_unit_score > self.previous_killed_unit_score:\n",
    "                reward += KILL_UNIT_REWARD\n",
    "                    \n",
    "            if killed_building_score > self.previous_killed_building_score:\n",
    "                reward += KILL_BUILDING_REWARD\n",
    "                \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, str(current_state))\n",
    "        \n",
    "        rl_action = self.qlearn.choose_action(str(current_state))\n",
    "        smart_action = smart_actions[rl_action]\n",
    "        \n",
    "        self.previous_killed_unit_score = killed_unit_score\n",
    "        self.previous_killed_building_score = killed_building_score\n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = rl_action\n",
    "        \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "        \n",
    "        if smart_action == ACTION_DO_NOTHING:\n",
    "            return actions.FUNCTIONS.no_op()\n",
    "\n",
    "        elif smart_action == ACTION_SELECT_SCV:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                if len(scvs) > 0:\n",
    "                    scv = random.choice(scvs)\n",
    "                    if scv.x >= 0 and scv.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformDistance(int(mean_x), 0, int(mean_y), 20)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target)        \n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "                if len(ccs) > 0:\n",
    "                    mean_x, mean_y = self.getMeanLocation(ccs)\n",
    "                    target = self.transformDistance(int(mean_x), 20, int(mean_y), 0)\n",
    "\n",
    "                    return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "        elif smart_action == ACTION_SELECT_BARRACKS:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                if len(barracks) > 0:\n",
    "                    barrack = random.choice(barracks)\n",
    "                    if barrack.x >= 0 and barrack.y >= 0:\n",
    "                        return actions.FUNCTIONS.select_point(\"select\", (barrack.x,\n",
    "                                                                              barrack.y))\n",
    "        \n",
    "        elif smart_action == ACTION_BUILD_MARINE:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "        elif smart_action == ACTION_SELECT_ARMY:\n",
    "            if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        elif smart_action == ACTION_ATTACK:\n",
    "            #if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "            if not self.unit_type_is_selected(obs, units.Terran.SCV) and self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x), int(y)))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0827 13:40:14.084501 4423970240 sc_process.py:135] Launching SC2: /Applications/StarCraft II/Versions/Base81102/SC2.app/Contents/MacOS/SC2 -listen 127.0.0.1 -port 21377 -dataDir /Applications/StarCraft II/ -tempDir /var/folders/kl/h0d5qxj551x0d2y091w17l1h0000gn/T/sc-cex3_4wt/ -displayMode 0 -windowwidth 640 -windowheight 480 -windowx 50 -windowy 50\n",
      "I0827 13:40:14.112885 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 0, running: True\n",
      "I0827 13:40:15.122327 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 1, running: True\n",
      "I0827 13:40:16.125180 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 2, running: True\n",
      "I0827 13:40:17.131703 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 3, running: True\n",
      "I0827 13:40:18.134363 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 4, running: True\n",
      "I0827 13:40:19.140396 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 5, running: True\n",
      "I0827 13:40:20.147212 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 6, running: True\n",
      "I0827 13:40:21.151141 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 7, running: True\n",
      "I0827 13:40:22.157793 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 8, running: True\n",
      "I0827 13:40:23.159646 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 9, running: True\n",
      "I0827 13:40:24.166495 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 10, running: True\n",
      "I0827 13:40:25.172230 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 11, running: True\n",
      "I0827 13:40:26.175658 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 12, running: True\n",
      "I0827 13:40:27.181591 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 13, running: True\n",
      "I0827 13:40:28.183754 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 14, running: True\n",
      "I0827 13:40:29.190640 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 15, running: True\n",
      "I0827 13:40:30.197548 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 16, running: True\n",
      "I0827 13:40:31.201606 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 17, running: True\n",
      "I0827 13:40:32.206058 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 18, running: True\n",
      "I0827 13:40:33.210328 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 19, running: True\n",
      "I0827 13:40:34.215663 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 20, running: True\n",
      "I0827 13:40:35.217999 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 21, running: True\n",
      "I0827 13:40:36.220614 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 22, running: True\n",
      "I0827 13:40:37.226672 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 23, running: True\n",
      "I0827 13:40:38.233577 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 24, running: True\n",
      "I0827 13:40:39.240485 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 25, running: True\n",
      "I0827 13:40:40.246176 4423970240 remote_controller.py:166] Connecting to: ws://127.0.0.1:21377/sc2api, attempt: 26, running: True\n",
      "I0827 13:40:47.927109 4423970240 sc2_env.py:314] Environment is ready\n",
      "I0827 13:40:47.943653 4423970240 sc2_env.py:506] Starting episode 1: [terran, random] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0/no_op                                              ()\n",
      "   1/move_camera                                        (1/minimap [64, 64])\n",
      "   2/select_point                                       (6/select_point_act [4]; 0/screen [84, 84])\n",
      "   3/select_rect                                        (7/select_add [2]; 0/screen [84, 84]; 2/screen2 [84, 84])\n",
      "   4/select_control_group                               (4/control_group_act [5]; 5/control_group_id [10])\n",
      " 264/Harvest_Gather_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      "  12/Attack_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
      "  13/Attack_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
      " 269/Harvest_Return_quick                               (3/queued [2])\n",
      " 274/HoldPosition_quick                                 (3/queued [2])\n",
      " 549/Effect_Spray_minimap                               (3/queued [2]; 1/minimap [64, 64])\n",
      "  44/Build_CommandCenter_screen                         (3/queued [2]; 0/screen [84, 84])\n",
      "  50/Build_EngineeringBay_screen                        (3/queued [2]; 0/screen [84, 84])\n",
      " 451/Smart_screen                                       (3/queued [2]; 0/screen [84, 84])\n",
      " 452/Smart_minimap                                      (3/queued [2]; 1/minimap [64, 64])\n",
      " 453/Stop_quick                                         (3/queued [2])\n",
      " 331/Move_screen                                        (3/queued [2]; 0/screen [84, 84])\n",
      " 332/Move_minimap                                       (3/queued [2]; 1/minimap [64, 64])\n",
      " 333/Patrol_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
      " 334/Patrol_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
      "  79/Build_Refinery_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      "  91/Build_SupplyDepot_screen                           (3/queued [2]; 0/screen [84, 84])\n",
      " 220/Effect_Repair_screen                               (3/queued [2]; 0/screen [84, 84])\n",
      " 221/Effect_Repair_autocast                             ()\n",
      " 230/Effect_Spray_screen                                (3/queued [2]; 0/screen [84, 84])\n",
      " 261/Halt_quick                                         (3/queued [2])\n",
      "  42/Build_Barracks_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      " 140/Cancel_quick                                       (3/queued [2])\n",
      " 335/Rally_Units_screen                                 (3/queued [2]; 0/screen [84, 84])\n",
      " 336/Rally_Units_minimap                                (3/queued [2]; 1/minimap [64, 64])\n",
      " 281/Lift_quick                                         (3/queued [2])\n",
      " 477/Train_Marine_quick                                 (3/queued [2])\n",
      "   6/select_idle_worker                                 (10/select_worker [4])\n",
      " 168/Cancel_Last_quick                                  (3/queued [2])\n",
      "  11/build_queue                                        (11/build_queue_id [10])\n",
      "   7/select_army                                        (7/select_add [2])\n",
      "  43/Build_Bunker_screen                                (3/queued [2]; 0/screen [84, 84])\n",
      "   5/select_unit                                        (8/select_unit_act [4]; 9/select_unit_id [500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0827 13:41:39.752333 4423970240 sc2_env.py:722] Episode 1 finished after 17024 game steps. Outcome: [-1], reward: [-1], score: [7705]\n",
      "I0827 13:41:43.965456 4423970240 sc2_env.py:506] Starting episode 2: [terran, random] on Simple64\n",
      "I0827 13:42:56.711888 4423970240 sc2_env.py:722] Episode 2 finished after 21952 game steps. Outcome: [-1], reward: [-1], score: [8550]\n",
      "I0827 13:43:00.968086 4423970240 sc2_env.py:506] Starting episode 3: [terran, random] on Simple64\n",
      "I0827 13:43:01.929543 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.930707 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.931674 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.934917 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.935953 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.936646 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.938066 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.945103 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.946482 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:43:01.947756 123145503141888 sc2_env.py:752] Environment Close\n",
      "I0827 13:44:13.577691 4423970240 sc2_env.py:722] Episode 3 finished after 16544 game steps. Outcome: [-1], reward: [-1], score: [6495]\n",
      "I0827 13:44:18.110351 4423970240 sc2_env.py:506] Starting episode 4: [terran, random] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 229.862 seconds for 7722 steps: 33.594 fps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0827 13:44:43.316385 4423970240 sc2_env.py:752] Environment Close\n",
      "I0827 13:44:43.319229 4423970240 sc_process.py:232] Shutdown gracefully.\n",
      "I0827 13:44:43.319980 4423970240 sc_process.py:210] Shutdown with return code: 1\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Error during save_replay: Socket error: [Errno 54] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/protocol.py\u001b[0m in \u001b[0;36mcatch_websocket_connection_errors\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mwebsocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWebSocketConnectionClosedException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/protocol.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mcatch_websocket_connection_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mresponse_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mopcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mABNF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPCODE_TEXT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_data\u001b[0;34m(self, control_frame)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_data_frame\u001b[0;34m(self, control_frame)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_received_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_strict\u001b[0;34m(self, bufsize)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# fragmentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mbytes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, bufsize)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mWebSocketConnectionClosedException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_socket.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(sock, bufsize)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mbytes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/websocket/_socket.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLWantReadError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/protocol.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_req\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/protocol.py\u001b[0m in \u001b[0;36msend_req\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/stopwatch.py\u001b[0m in \u001b[0;36m_stopwatch\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_stopwatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/protocol.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc2_verbose_protocol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/protocol.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mcatch_websocket_connection_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mresponse_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/protocol.py\u001b[0m in \u001b[0;36mcatch_websocket_connection_errors\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Socket error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: Socket error: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-60174cc17dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e38c1d5d9a0b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m   \u001b[0mrun_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e38c1d5d9a0b>\u001b[0m in \u001b[0;36mrun_thread\u001b[0;34m(agent_classes, players, map_name, visualize)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mrun_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_agent_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munused_argv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/env/base_env_wrapper.py\u001b[0m in \u001b[0;36msave_replay\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/env/sc2_env.py\u001b[0m in \u001b[0;36msave_replay\u001b[0;34m(self, replay_dir, prefix)\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     replay_path = self._run_config.save_replay(\n\u001b[0;32m--> 747\u001b[0;31m         self._controllers[0].save_replay(), replay_dir, prefix)\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrote replay to: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreplay_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/remote_controller.py\u001b[0m in \u001b[0;36m_valid_status\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \"`%s` called while in state: %s, valid: (%s)\" % (\n\u001b[1;32m     98\u001b[0m                 func.__name__, self.status, \",\".join(map(str, valid))))\n\u001b[0;32m---> 99\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_valid_status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/stopwatch.py\u001b[0m in \u001b[0;36m_stopwatch\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_stopwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_stopwatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/remote_controller.py\u001b[0m in \u001b[0;36msave_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;34m\"\"\"Save a replay, returning the data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc_pb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestSaveReplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/pysc2/lib/protocol.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_req\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error during %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       raise ConnectionError(\n",
      "\u001b[0;31mConnectionError\u001b[0m: Error during save_replay: Socket error: [Errno 54] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Winning rate graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCORE_FILE + '.txt', \"rb\") as fp:\n",
    "    scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0. 100.]\n",
      " [  0.   0. 100.]\n",
      " [  0.   0. 100.]\n",
      " [  0.  20.  80.]\n",
      " [ 10.  20.  70.]]\n"
     ]
    }
   ],
   "source": [
    "np_scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[0], color='r', label='win rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[1], color='g', label='tie rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[2], color='b', label='lose rate')\n",
    "plt.ylabel('Score %')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 100.0], [0.0, 0.0, 100.0], [0.0, 0.0, 100.0]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5,) and (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fb892bf35429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'win rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tie rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lose rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2824\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (3,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [[0.0, 0.0, 100.0], [0.0, 0.0, 100.0], [0.0, 0.0, 100.0], [0.0, 20.0, 80.0], [10.0, 20.0, 70.0]]\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores[0], color='r', label='win rate')\n",
    "plt.plot(np.arange(len(scores)), scores[1], color='g', label='tie rate')\n",
    "plt.plot(np.arange(len(scores)), scores[2], color='b', label='lose rate')\n",
    "plt.ylabel('Score %')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcraft2",
   "language": "python",
   "name": "starcraft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
